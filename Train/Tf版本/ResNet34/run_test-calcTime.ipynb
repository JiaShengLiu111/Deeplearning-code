{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动加载已修改的python文件\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import utils as utils\n",
    "import tensorflow as tf  \n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.metrics import accuracy_score   \n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'       # 使用第二块GPU（从0开始）\n",
    "\n",
    "pwd = \"/notebooks/17_LJS/TbsData/\"  # 当前路径 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取所有样本路径\n",
    "def getSamplesDir():  \n",
    "    image_junk = []  # 垃圾样本\n",
    "    image_normal = []  # 正常样本\n",
    "    image_abnormal = []  # 异常样本\n",
    "    image_total = []  \n",
    "    \n",
    "\n",
    "    imageJunk_dir = pwd+\"TBS3/Junk\"\n",
    "    imageNormal_dir = pwd+\"TBS3/Negative\"\n",
    "    imageAbnormal_dir = pwd+\"TBS3/Positive\"\n",
    "    # imageAscus_dir = pwd+\"TBS3/ASCUS/Positive\"\n",
    "\n",
    "    # 获取所有样本名称\n",
    "    listJunk = os.listdir(imageJunk_dir)\n",
    "    listNormal = os.listdir(imageNormal_dir)\n",
    "    listAbnormal = os.listdir(imageAbnormal_dir)\n",
    "    # listAscus = os.listdir(imageAscus_dir)\n",
    "    \n",
    "    # 路径+名称\n",
    "    listJunk = [os.path.join(imageJunk_dir,listJunk[i]) for i in range(len(listJunk))]\n",
    "    listNormal = [os.path.join(imageNormal_dir,listNormal[i]) for i in range(len(listNormal))]\n",
    "    listAbnormal = [os.path.join(imageAbnormal_dir,listAbnormal[i]) for i in range(len(listAbnormal))] \n",
    "    # listAscus = [os.path.join(imageAscus_dir,listAscus[i]) for i in range(len(listAscus))]\n",
    "    \n",
    "    # image_total = listJunk+listNormal+listAbnormal+listAscus\n",
    "    # return image_total,len(listJunk),len(listNormal),len(listAbnormal)+len(listAscus)  \n",
    "    image_total = listJunk+listNormal+listAbnormal\n",
    "    return image_total,len(listJunk),len(listNormal),len(listAbnormal)  \n",
    "\n",
    "# 读取原始测试集\n",
    "# 获取所有样本路径\n",
    "def getSamplesDir_test():  \n",
    "    image_junk = []  # 垃圾样本\n",
    "    image_normal = []  # 正常样本\n",
    "    image_abnormal = []  # 异常样本\n",
    "    image_total = []  \n",
    "\n",
    "    imageJunk_dir = pwd+\"TBS3/TestEnhance/Junk\"\n",
    "    imageNormal_dir = pwd+\"TBS3/TestEnhance/Negative\"\n",
    "    imageAbnormal_dir = pwd+\"TBS3/TestEnhance/Positive\"\n",
    "\n",
    "    # 获取所有样本名称\n",
    "    listJunk = os.listdir(imageJunk_dir)\n",
    "    listNormal = os.listdir(imageNormal_dir)\n",
    "    listAbnormal = os.listdir(imageAbnormal_dir)\n",
    "    \n",
    "    # 路径+名称\n",
    "    listJunk = [os.path.join(imageJunk_dir,listJunk[i]) for i in range(len(listJunk))]\n",
    "    listNormal = [os.path.join(imageNormal_dir,listNormal[i]) for i in range(len(listNormal))]\n",
    "    listAbnormal = [os.path.join(imageAbnormal_dir,listAbnormal[i]) for i in range(len(listAbnormal))] \n",
    "    \n",
    "    image_total = listJunk+listNormal+listAbnormal\n",
    "    return image_total,len(listJunk),len(listNormal),len(listAbnormal)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改，在训练样本的同时分批次的读取样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取所有样本的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数据量为：\n",
      "(425122,)\n",
      "其它样本数为：\n",
      "111486\n",
      "正常样本数为：\n",
      "127228\n",
      "异常样本数为：\n",
      "186408\n"
     ]
    }
   ],
   "source": [
    "allSamplesDir,junkLen,negativeLen,positiveLen = getSamplesDir()  # 读取样本  \n",
    "X_test,junkLen_test,negativeLen_test,positiveLen_test = getSamplesDir_test()  # 读取测试集\n",
    "\n",
    "# 将两部分样本加在一块\n",
    "allSamplesDir = allSamplesDir + X_test\n",
    "junkLen = junkLen + junkLen_test\n",
    "negativeLen = negativeLen + negativeLen_test\n",
    "positiveLen = positiveLen + positiveLen_test\n",
    "\n",
    "datas = np.array(allSamplesDir)\n",
    "print (\"样本总数据量为：\")\n",
    "print datas.shape\n",
    "\n",
    "print \"其它样本数为：\"\n",
    "print junkLen\n",
    "print \"正常样本数为：\"\n",
    "print negativeLen\n",
    "print \"异常样本数为：\"\n",
    "print positiveLen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解决验证集、测试集被增强的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "其它样本数:55743\n",
      "正常样本数:31807\n",
      "异常样本数:23301\n",
      "未增强前_所有样本的个数:110851\n"
     ]
    }
   ],
   "source": [
    "allSamplesDir_NoEnhance = [] # 未进行数据增强的原始数据\n",
    "\n",
    "# 未进行数据增强的原始数据中各类数据的数目\n",
    "negativeSize_noEnhance = 0  \n",
    "positiveSize_noEnhance = 0\n",
    "junkSize_noEnhance = 0\n",
    "\n",
    "for i in range(len(allSamplesDir)):\n",
    "    if allSamplesDir[i].find('0.jpg')>=0 and allSamplesDir[i].find('Negative')>=0:  # 未增强数据：正常样本类别\n",
    "        negativeSize_noEnhance = negativeSize_noEnhance+1 \n",
    "        allSamplesDir_NoEnhance.append(allSamplesDir[i])\n",
    "    \n",
    "    if allSamplesDir[i].find('0.jpg')>=0 and allSamplesDir[i].find('Positive')>=0:  # 未增强数据：异常样本类别\n",
    "        positiveSize_noEnhance = positiveSize_noEnhance+1\n",
    "        allSamplesDir_NoEnhance.append(allSamplesDir[i])\n",
    "        \n",
    "    if allSamplesDir[i].find('0.jpg')>=0 and allSamplesDir[i].find('Junk')>=0:  # 未增强数据：其它样本类别\n",
    "        junkSize_noEnhance = junkSize_noEnhance+1\n",
    "        allSamplesDir_NoEnhance.append(allSamplesDir[i])\n",
    "\n",
    "print \"其它样本数:\"+str(junkSize_noEnhance)\n",
    "print \"正常样本数:\"+str(negativeSize_noEnhance)\n",
    "print \"异常样本数:\"+str(positiveSize_noEnhance)\n",
    "\n",
    "print \"未增强前_所有样本的个数:\"+str(negativeSize_noEnhance+positiveSize_noEnhance+junkSize_noEnhance) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为“未增强前数据”生成样本标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneLabel(allSamplesDir_NoEnhance):\n",
    "    y = []\n",
    "    for i in range(len(allSamplesDir_NoEnhance)):\n",
    "        value = allSamplesDir_NoEnhance[i]\n",
    "        if value.find('Junk')>=0:\n",
    "            y.append([1,0,0])\n",
    "        elif value.find('Negative')>=0:\n",
    "            y.append([0,1,0])\n",
    "        elif value.find('Positive')>=0:\n",
    "            y.append([0,0,1])\n",
    "        else:\n",
    "            print(\"geneLabel函数中出现了无法解决的bug！\") \n",
    "    return y\n",
    "\n",
    "y = geneLabel(allSamplesDir_NoEnhance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 按照样本统计各样本所对应的图片个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 提取一个图片路径中样本标签的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入：/notebooks/17_LJS/TbsData/TBS3/Junk/EYC37815_C000076C_0.jpg\n",
    "# 输出：EYC37815\n",
    "def getSampleBelong(fullpath):\n",
    "    num=0  # fullpath中/的个数\n",
    "    for i in range(len(fullpath)):\n",
    "        if fullpath[i]=='/':\n",
    "            num=num+1\n",
    "    # print \"num:\"+str(num)\n",
    "    \n",
    "    index = 0 # 统计遍历中遇到的/个数\n",
    "    for i in range(len(fullpath)):\n",
    "        if fullpath[i]=='/':\n",
    "            index=index+1\n",
    "            if index==num:\n",
    "                break\n",
    "    \n",
    "    result = ''\n",
    "    for i in range(i+1,len(fullpath)):\n",
    "        if fullpath[i]!='_':\n",
    "            result=result+fullpath[i]\n",
    "        else:\n",
    "            break\n",
    "    # print \"result:\"+result\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Info:  # 样本标签信息\n",
    "    def __init__(self):\n",
    "        self.sampleName = ''     # 样本标签名称\n",
    "        self.sampleNum = 0     # 样本标签对应的图片个数 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 统计各样本所对应的图片个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "countResult = []  # 统计结果\n",
    "for i in range(len(allSamplesDir_NoEnhance)):\n",
    "    sample = getSampleBelong(allSamplesDir_NoEnhance[i])  # 某一个样本标签名称\n",
    "    # print \"sample:\"+sample\n",
    "    \n",
    "    # 判断sample是否在countResult中\n",
    "    num=0 # 用于计数\n",
    "    for j in range(len(countResult)): \n",
    "        if countResult[j].sampleName==sample: \n",
    "            break\n",
    "        num=num+1\n",
    "    # print \"len(countResult):\"+str(len(countResult))\n",
    "    # print \"j:\"+str(j)\n",
    "    \n",
    "    if num>=len(countResult):  # 表示sample不在countResult中\n",
    "        info_tmp = Info()\n",
    "        info_tmp.sampleName = sample\n",
    "        info_tmp.sampleNum  =1\n",
    "        countResult.append(info_tmp)\n",
    "    else:\n",
    "        countResult[j].sampleNum = countResult[j].sampleNum+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 输出统计信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EYC37815\t\t390\n",
      "ESY51427\t\t227\n",
      "EES57040\t\t813\n",
      "EHG81734\t\t439\n",
      "C40655-ASCUS\t\t3918\n",
      "218327\t\t353\n",
      "223248\t\t1085\n",
      "EYC37951\t\t923\n",
      "EYC37760\t\t691\n",
      "B007852-ISIL\t\t984\n",
      "222488-ISIL\t\t2939\n",
      "C40740-ASCUS\t\t5858\n",
      "B007497-ISIL\t\t1045\n",
      "C41911-ASCUS\t\t720\n",
      "219352-ISIL\t\t802\n",
      "WHN806071\t\t457\n",
      "223115-ASC-H\t\t601\n",
      "222267-ISIL\t\t79\n",
      "EHG81280\t\t451\n",
      "EES57430\t\t676\n",
      "WQS805029\t\t384\n",
      "EHG81189\t\t482\n",
      "B007400-ISIL\t\t1651\n",
      "EHG81113\t\t641\n",
      "216762\t\t168\n",
      "EES109493\t\t392\n",
      "GL801153\t\t400\n",
      "CA2657-ASCUS\t\t410\n",
      "C40741-ASCUS\t\t1615\n",
      "222113-ISIL\t\t538\n",
      "ESY51306\t\t655\n",
      "GL801272\t\t453\n",
      "218710\t\t395\n",
      "GL801145\t\t602\n",
      "C40531-ASCUS\t\t912\n",
      "223744-ASC-H\t\t322\n",
      "ESY51668\t\t779\n",
      "CA0741-ASCUS\t\t273\n",
      "EES57484\t\t561\n",
      "EHG81624\t\t380\n",
      "C42441-ASCUS\t\t525\n",
      "C40592-ASCUS\t\t861\n",
      "ESY51750\t\t871\n",
      "EES57361\t\t990\n",
      "C44346\t\t996\n",
      "GL801175\t\t585\n",
      "EES57376\t\t1029\n",
      "223657\t\t32\n",
      "223648\t\t36\n",
      "C42404-ASCUS\t\t731\n",
      "B004562\t\t112\n",
      "B005801\t\t290\n",
      "EHG81910\t\t480\n",
      "B008053-LSIL-2\t\t499\n",
      "221960-ISIL\t\t432\n",
      "B003802\t\t369\n",
      "218326\t\t21\n",
      "223523\t\t162\n",
      "C25811\t\t434\n",
      "EYC37755\t\t650\n",
      "EYC37753\t\t800\n",
      "219073\t\t103\n",
      "218967-ISIL\t\t152\n",
      "CA2463-ASCUS\t\t241\n",
      "CA2564-ASCUS\t\t102\n",
      "ESY51300\t\t334\n",
      "B006968-ISIL\t\t201\n",
      "EYC37878\t\t674\n",
      "ESY51432\t\t580\n",
      "C40318\t\t289\n",
      "EHG81319\t\t459\n",
      "WHS813233\t\t214\n",
      "223316-ASC-H\t\t243\n",
      "222121-ISIL\t\t227\n",
      "219262\t\t405\n",
      "217685-ISIL\t\t428\n",
      "C40162-ASCUS\t\t348\n",
      "222539-ISIL\t\t69\n",
      "ESY51034\t\t523\n",
      "ESY51357\t\t887\n",
      "WHS813137\t\t247\n",
      "C42052-ASCUS\t\t91\n",
      "EYC37859\t\t143\n",
      "WQS805065\t\t375\n",
      "223650\t\t377\n",
      "EHG80744\t\t402\n",
      "B006413\t\t92\n",
      "WXZ811621\t\t298\n",
      "217641-ISIL\t\t332\n",
      "223652\t\t20\n",
      "219333-ISIL\t\t65\n",
      "EES55799\t\t1309\n",
      "B007935-LSIL\t\t97\n",
      "ESY51284\t\t374\n",
      "EHG81740\t\t387\n",
      "C29470\t\t146\n",
      "223039-ASC-H\t\t119\n",
      "C44877-ASC-H\t\t186\n",
      "EYC37740\t\t393\n",
      "B005846\t\t32\n",
      "221761-ISIL\t\t214\n",
      "C40510-ASCUS\t\t28\n",
      "ESY51050\t\t411\n",
      "223649\t\t155\n",
      "EHG81084\t\t370\n",
      "EHG81039\t\t329\n",
      "EHG81318\t\t427\n",
      "CA2576-ASCUS\t\t346\n",
      "C33539\t\t180\n",
      "WXZ811571\t\t637\n",
      "ESY51768\t\t647\n",
      "221558\t\t41\n",
      "223654\t\t112\n",
      "ESY51446\t\t362\n",
      "WXZ811593\t\t230\n",
      "EHG81037\t\t336\n",
      "C42206-ASCUS\t\t156\n",
      "218842-LSIL\t\t90\n",
      "EHG80892\t\t242\n",
      "223658\t\t24\n",
      "EHG81098\t\t231\n",
      "223659\t\t57\n",
      "217920-ISIL\t\t46\n",
      "B009029\t\t43\n",
      "223522\t\t177\n",
      "EXY15266\t\t355\n",
      "B006257\t\t48\n",
      "223656\t\t71\n",
      "223832\t\t54\n",
      "B008071-LSIL\t\t36\n",
      "222047-ISIL\t\t64\n",
      "B007184\t\t45\n",
      "EES109654\t\t372\n",
      "WXZ809102\t\t570\n",
      "EXN06265\t\t205\n",
      "EES109925\t\t579\n",
      "217766\t\t72\n",
      "215754\t\t25\n",
      "C40544-ASCUS\t\t52\n",
      "EYC37866\t\t34\n",
      "222818\t\t110\n",
      "B005284\t\t26\n",
      "219053\t\t21\n",
      "223651\t\t79\n",
      "CA2449-ASCUS\t\t24\n",
      "WHS813223\t\t159\n",
      "210569\t\t22\n",
      "EYC37726\t\t343\n",
      "223653\t\t11\n",
      "223191-ASC-H\t\t39\n",
      "B007267\t\t5\n",
      "WQS802691\t\t372\n",
      "WHS813111\t\t294\n",
      "C42064-ASCUS\t\t29\n",
      "EES110715\t\t350\n",
      "222126\t\t14\n",
      "EXN16676\t\t93\n",
      "EXY15343\t\t268\n",
      "223646\t\t15\n",
      "C45054-ASC-H-2\t\t11\n",
      "WQS802875\t\t466\n",
      "EES109658\t\t306\n",
      "WXZ809042\t\t497\n",
      "EXN16796\t\t244\n",
      "EES109726\t\t354\n",
      "C40423-ASCUS\t\t14\n",
      "EXY15577\t\t220\n",
      "217576-ISIL\t\t21\n",
      "221682\t\t12\n",
      "C40898-ASCUS\t\t4\n",
      "217683\t\t18\n",
      "WXZ809054\t\t124\n",
      "EES109907\t\t392\n",
      "C37037\t\t6\n",
      "223655\t\t3\n",
      "WWC808210\t\t109\n",
      "EES110049\t\t390\n",
      "C40047\t\t3\n",
      "222034\t\t2\n",
      "EES110045\t\t178\n",
      "EES110090\t\t190\n",
      "EES110097\t\t237\n",
      "WHN803703\t\t41\n",
      "EYC00721\t\t146\n",
      "EXY15412\t\t230\n",
      "EXG02207\t\t128\n",
      "EXG02700\t\t298\n",
      "EXG02775\t\t271\n",
      "EXG44534\t\t236\n",
      "EXY17778\t\t208\n",
      "EXG01372\t\t106\n",
      "EXY15279\t\t311\n",
      "WHS813094\t\t237\n",
      "EXY1774110\t\t180\n",
      "EXY17773\t\t259\n",
      "EHG81531\t\t151\n",
      "WQS802794\t\t270\n",
      "EHG81483\t\t96\n",
      "EXY15965\t\t162\n",
      "ESY35124\t\t144\n",
      "EXY15799\t\t141\n",
      "EXY17580\t\t306\n",
      "EXN02946\t\t228\n",
      "EXY15659\t\t85\n",
      "EXN03046\t\t187\n",
      "EXY15073\t\t204\n",
      "EXG01514\t\t303\n",
      "EXG01569\t\t264\n",
      "EXY17763\t\t140\n",
      "EXG02460\t\t101\n",
      "ESZ03346\t\t85\n",
      "EXG01521\t\t196\n",
      "EXG01453\t\t143\n",
      "ESZ04745\t\t193\n",
      "EXY15953\t\t109\n",
      "WHN804021\t\t285\n",
      "EXY15827\t\t308\n",
      "EXY16733\t\t263\n",
      "EXG02407\t\t86\n",
      "EQJ01294\t\t170\n",
      "EXG02771\t\t221\n",
      "EHG43637\t\t267\n",
      "EXY17709\t\t202\n",
      "EXG44501\t\t152\n",
      "EXY17538\t\t244\n",
      "EHG45572\t\t243\n",
      "ESY52283\t\t274\n",
      "EYC00749\t\t198\n",
      "EXG02294\t\t107\n",
      "EXN16834\t\t283\n",
      "EXY17810\t\t137\n",
      "EXN16735\t\t341\n",
      "EYC00744\t\t224\n",
      "EXY14557\t\t286\n",
      "EXG02607\t\t151\n",
      "EXN16602\t\t107\n",
      "EQJ01112\t\t71\n",
      "EXY15248\t\t105\n",
      "ESZ05518\t\t95\n",
      "EXY17652\t\t317\n",
      "YC706217\t\t166\n",
      "EXY16884\t\t447\n",
      "EXG30461\t\t280\n",
      "ESY11048\t\t155\n",
      "EXY17633\t\t169\n",
      "ESY06439\t\t148\n",
      "EXY15002\t\t154\n",
      "EYC01633\t\t185\n",
      "EHG45565\t\t296\n",
      "EXG44506\t\t269\n",
      "EYC37734\t\t20\n",
      "EYC01691\t\t183\n",
      "EXY16638\t\t338\n",
      "EHG43850\t\t180\n",
      "EXG02699\t\t183\n",
      "EYC00756\t\t196\n",
      "ESZ05176\t\t104\n",
      "EJM13136\t\t72\n",
      "EXN03228\t\t262\n",
      "ESY51640\t\t122\n",
      "EXG01450\t\t88\n",
      "EXY16419\t\t160\n",
      "ESY35110\t\t99\n",
      "EXN02107\t\t257\n",
      "ESZ05192\t\t115\n",
      "EES110120\t\t286\n",
      "EXY16334\t\t136\n",
      "EXY17653\t\t173\n",
      "EXN02894\t\t373\n",
      "EXG29848\t\t119\n",
      "EJM13034\t\t108\n",
      "EXG29860\t\t157\n",
      "EXY16163\t\t258\n",
      "EXN02991\t\t175\n",
      "EQJ00952\t\t124\n",
      "GL801224\t\t115\n",
      "EXN03225\t\t141\n",
      "ESY11043\t\t166\n",
      "EXY15787\t\t143\n",
      "ESY52288\t\t90\n",
      "EXN03221\t\t78\n",
      "EQJ00917\t\t132\n",
      "EQJ01272\t\t134\n",
      "ESZ04727\t\t116\n",
      "ESY51834\t\t182\n",
      "EXN16380\t\t147\n",
      "EQJ00918\t\t94\n",
      "EXN03059\t\t113\n",
      "EXY14606\t\t70\n",
      "EXG02247\t\t216\n",
      "EXY14175\t\t67\n",
      "EXN02716\t\t308\n",
      "EXY15517\t\t459\n",
      "EYC21972\t\t607\n",
      "EYC21817\t\t237\n",
      "EES110672\t\t234\n",
      "EYC21715\t\t550\n",
      "EYC02893\t\t176\n",
      "EES110518\t\t164\n",
      "EES74569\t\t180\n",
      "EES73182\t\t191\n",
      "EYC02925\t\t581\n",
      "EES110146\t\t183\n",
      "EYC02695\t\t229\n",
      "EYC23368\t\t136\n",
      "EES110558\t\t242\n",
      "EYC37704\t\t232\n",
      "EYC02730\t\t208\n",
      "EES110539\t\t183\n",
      "EYC21966\t\t64\n",
      "EES55648\t\t822\n",
      "EYC01725\t\t32\n",
      "EYC37539\t\t150\n",
      "EES73708\t\t288\n",
      "EES55299\t\t776\n",
      "EES54858\t\t205\n",
      "EYC37441\t\t59\n",
      "EES55691\t\t769\n",
      "EYC02049\t\t123\n",
      "EYC02796\t\t398\n",
      "EYC21845\t\t114\n",
      "EYC01712\t\t51\n",
      "EES73439\t\t130\n",
      "EES58627\t\t108\n",
      "EYC37648\t\t43\n",
      "EYC02707\t\t43\n",
      "EYC01927\t\t8\n",
      "EYC02038\t\t23\n",
      "EES74467\t\t132\n",
      "EYC37568\t\t68\n",
      "EHG21959\t\t297\n",
      "EYC01759\t\t36\n",
      "EES56918\t\t171\n",
      "WHN803847\t\t25\n",
      "EYC37679\t\t9\n",
      "EES74504\t\t76\n",
      "EES110494\t\t113\n",
      "EES55940\t\t158\n",
      "WDH809322\t\t11\n",
      "WHN803683\t\t3\n",
      "EYC02735\t\t26\n",
      "EES55760\t\t796\n",
      "EES58637\t\t64\n",
      "EES59476\t\t77\n",
      "EES110528\t\t107\n",
      "WHN803884\t\t1\n",
      "D15158\t\t749\n",
      "222267\t\t946\n",
      "EHG42029\t\t497\n",
      "EHG41879\t\t872\n",
      "C42317\t\t232\n",
      "C42064\t\t400\n",
      "EHG41813\t\t551\n",
      "C42052\t\t226\n",
      "B007643\t\t187\n",
      "EHG42136\t\t198\n",
      "EES55288\t\t309\n",
      "EHG42884\t\t138\n",
      "EHG42491\t\t356\n",
      "EYC02689\t\t437\n",
      "EYC02572\t\t193\n",
      "EYC02452\t\t14\n",
      "EYC02241\t\t43\n",
      "EYC02059\t\t49\n",
      "EYC02060\t\t9\n",
      "EYC02547\t\t125\n",
      "EYC02508\t\t47\n",
      "EYC02050\t\t22\n",
      "EYC02546\t\t37\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(countResult)): \n",
    "    print countResult[i].sampleName+\"\\t\\t\"+str(countResult[i].sampleNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证未进行图片增强前_图片的总数为：110851\n"
     ]
    }
   ],
   "source": [
    "allPictureNum = sum([countResult[i].sampleNum for i in range(len(countResult))])\n",
    "print \"验证未进行图片增强前_图片的总数为：\"+str(allPictureNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 将countResult打乱顺序，注意shuffle每次打乱的顺序都不一样，但此处使用的是train_test_split函数，可以指定种子点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myShuffle(inputs,random_state=888):\n",
    "    X_train,_,_,_ = train_test_split(inputs,[1]*len(inputs),test_size = 0,random_state =random_state)\n",
    "    return X_train\n",
    "\n",
    "# random.shuffle(countResult)\n",
    "countResult = myShuffle(countResult,random_state=20)  # 使用固定的种子点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 划分“未增强前数据”，训练集：验证集:测试集\n",
    "##### 测试集采用原始测试集等大，训练集：验证集=8:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num:77220.0\n",
      "val_num:19305.0\n",
      "test_num:14326\n",
      "\n",
      "数据集划分结果：\n",
      "X_train_num:77406\n",
      "X_val_num:19737\n",
      "X_test_num:13708\n",
      "\n",
      "训练集：验证集=3.92187262502\n"
     ]
    }
   ],
   "source": [
    "test_num = 14326\n",
    "train_num = 0.8*(allPictureNum-test_num)\n",
    "val_num = 0.2*(allPictureNum-test_num)\n",
    "\n",
    "print \"train_num:\"+str(train_num)\n",
    "print \"val_num:\"+str(val_num)\n",
    "print \"test_num:\"+str(test_num)\n",
    "\n",
    "\n",
    "def splitTrain_Test(countResult):\n",
    "    X_train = []\n",
    "    X_val = []\n",
    "    X_test = []\n",
    "\n",
    "    allPictureNum = sum([countResult[i].sampleNum for i in range(len(countResult))])  # 验证未进行图片增强前_图片的总数\n",
    "    train = 0  # 用于计数训练集中图片的个数\n",
    "    val = 0  # 用于计数验证集中图片的个数\n",
    "    test = 0  # 用于计数测试集中图片的个数\n",
    "    for i in range(len(countResult)):\n",
    "        if train<=train_num:\n",
    "            X_train.append(countResult[i])\n",
    "            train = train+countResult[i].sampleNum\n",
    "        elif val<=val_num:\n",
    "            X_val.append(countResult[i])\n",
    "            val = val+countResult[i].sampleNum\n",
    "        else:\n",
    "            X_test.append(countResult[i])\n",
    "            test = test+countResult[i].sampleNum\n",
    "    return X_train,X_val,X_test\n",
    "\n",
    "X_train_sampleName,X_val_sampleName,X_test_sampleName = splitTrain_Test(countResult) # 划分训练集：测试集=8:2 \n",
    "\n",
    "X_train_num = sum([X_train_sampleName[i].sampleNum for i in range(len(X_train_sampleName))])\n",
    "X_val_num = sum([X_val_sampleName[i].sampleNum for i in range(len(X_val_sampleName))])\n",
    "X_test_num = sum([X_test_sampleName[i].sampleNum for i in range(len(X_test_sampleName))])\n",
    "\n",
    "print \"\\n数据集划分结果：\"\n",
    "print \"X_train_num:\"+str(X_train_num)\n",
    "print \"X_val_num:\"+str(X_val_num)\n",
    "print \"X_test_num:\"+str(X_test_num)\n",
    "\n",
    "\n",
    "print \"\\n训练集：验证集=\"+str(float(X_train_num)/X_val_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 根据样本标签名称，生成标签名称所对应的图片路径（未经过数据增强）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断某个标签名称是否在某个Info列表中\n",
    "def inInfo(infos,sample):\n",
    "    num = 0\n",
    "    for j in range(len(infos)):\n",
    "        if infos[j].sampleName==sample:\n",
    "            break\n",
    "        num=num+1\n",
    "    if num<len(infos):  # 表示sample在infos中\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def getPictureRealPath(X_train_sampleName,X_val_sampleName,X_test_sampleName,allSamplesDir_NoEnhance):\n",
    "    X_train = []\n",
    "    X_val = []\n",
    "    X_test = []\n",
    "    \n",
    "    for i in range(len(allSamplesDir_NoEnhance)):\n",
    "        value = allSamplesDir_NoEnhance[i]  # 图片路径\n",
    "        sample = getSampleBelong(allSamplesDir_NoEnhance[i])  # value所对应的样本标签名称 \n",
    "        if inInfo(X_train_sampleName,sample)==True:\n",
    "            X_train.append(value)\n",
    "        elif inInfo(X_val_sampleName,sample)==True:\n",
    "            X_val.append(value)\n",
    "        elif inInfo(X_test_sampleName,sample)==True:\n",
    "            X_test.append(value) \n",
    "        else:\n",
    "            print(\"getPictureRealPath发生了无法解决的bug\")\n",
    "    return X_train,X_val,X_test\n",
    "\n",
    "X_train,X_val,X_test = getPictureRealPath(X_train_sampleName,X_val_sampleName,X_test_sampleName,allSamplesDir_NoEnhance) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 为训练集和验证集生成标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = geneLabel(X_train)\n",
    "y_val = geneLabel(X_val)\n",
    "y_test = geneLabel(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未增强前数据集总大小为：97143\n",
      "训练集大小：\n",
      "77406\n",
      "77406\n",
      "验证集大小：\n",
      "19737\n",
      "19737\n",
      "测试集大小：\n",
      "13708\n",
      "13708\n"
     ]
    }
   ],
   "source": [
    "print \"未增强前数据集总大小为：\"+str(len(X_train)+len(X_val))\n",
    "print (\"训练集大小：\")\n",
    "print len(X_train)\n",
    "print len(y_train) \n",
    "print(\"验证集大小：\")\n",
    "print len(X_val)\n",
    "print len(y_val)\n",
    "print(\"测试集大小：\")\n",
    "print len(X_test)\n",
    "print len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 统计“未增强前_训练集中各类数据的数量”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "垃圾样本：40951  阴性样本：20544  阳性样本：15911\n"
     ]
    }
   ],
   "source": [
    "def countSample(X_train):\n",
    "    junk = 0\n",
    "    negative = 0\n",
    "    positive = 0 \n",
    "    for i in range(len(X_train)):\n",
    "        value = X_train[i]\n",
    "        if value.find('Junk')>=0:\n",
    "            junk = junk+1\n",
    "        elif value.find('Negative')>=0:\n",
    "            negative = negative+1\n",
    "        else:\n",
    "            positive = positive+1 \n",
    "\n",
    "    return [junk,negative,positive]\n",
    "\n",
    "nums = countSample(X_train)\n",
    "print \"垃圾样本：\"+str(nums[0])+\"  阴性样本：\"+str(nums[1])+\"  阳性样本：\"+str(nums[2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 至此，得到“未增强前数据的验证集”，下面根据“未增强前数据的训练集”生成“数据增强之后的训练集”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注意：此处对Junk增强为原始1倍；Negative增强为原始2倍；Positive增强为原始3倍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集增强之后_大小为：122950\n"
     ]
    }
   ],
   "source": [
    "X_train_Enhance = []  # 表示进行数据增强后的训练集  \n",
    "\n",
    "positive_num = 0  # 对未增强前的阳性类别进行计数\n",
    "for i in range(len(X_train)):\n",
    "    value = X_train[i]\n",
    "    if value.find('Positive')>=0:  # 表示原始样本是“异常类别”\n",
    "        # 由于对阳性进行增强之后，样本量由过大，故选择性的对阳性进行数据增强\n",
    "        positive_num=positive_num+1 \n",
    "        if positive_num<=12500:  # 表示只对X_train中125000个样本进行数据增强\n",
    "            for j in range(3):\n",
    "                newValue = value.replace('0.jpg',str(j)+'.jpg')\n",
    "                X_train_Enhance.append(newValue)\n",
    "        else:\n",
    "            X_train_Enhance.append(value)\n",
    "    elif value.find('Negative')>=0:  # 表示原始样本是“正常类别”\n",
    "        newValue = value.replace('0.jpg',str(0)+'.jpg')\n",
    "        X_train_Enhance.append(newValue)\n",
    "        newValue = value.replace('0.jpg',str(1)+'.jpg')\n",
    "        X_train_Enhance.append(newValue)       \n",
    "        #         newValue = value.replace('0.jpg',str(2)+'.jpg')\n",
    "        #         X_train_Enhance.append(newValue)       \n",
    "        #         newValue = value.replace('0.jpg',str(5)+'.jpg')\n",
    "        #         X_train_Enhance.append(newValue)\n",
    "    elif value.find('Junk')>=0:  # 表示原始样本是“其它类别”\n",
    "        for j in range(1):\n",
    "            newValue = value.replace('0.jpg',str(j)+'.jpg')\n",
    "            X_train_Enhance.append(newValue)\n",
    "        \n",
    "        # 以前Junk类别没有经过增强。\n",
    "        # 对于“其它类别”，没有进行数据增强，原始样本和增强后的样本相同\n",
    "        # X_train_Enhance.append(value)\n",
    "    else:\n",
    "        print \"run中发生了无法结局的bug\"\n",
    "\n",
    "print (\"训练集增强之后_大小为：\")+str(len(X_train_Enhance))\n",
    "\n",
    "# random.shuffle(X_train_Enhance)  # 对X_train_Enhance进行乱序排列\n",
    "X_train_Enhance = myShuffle(X_train_Enhance,random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 根据“进行数据增强后的训练集”，生成对应的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train大小：122950\n",
      "y_train大小：122950\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_Enhance\n",
    "y_train = []\n",
    "for i in range(len(X_train)):\n",
    "    value = X_train[i]\n",
    "    if value.find('Junk')>=0:  # 正常样本类别\n",
    "        y_train.append([1,0,0])\n",
    "    elif value.find('Negative')>=0:  # 异常样本类别\n",
    "        y_train.append([0,1,0])\n",
    "    else:  # 其它样本类别\n",
    "        y_train.append([0,0,1])\n",
    "print (\"X_train大小：\")+str(len(X_train))\n",
    "print (\"y_train大小：\")+str(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数定义：随机从列表中截取指定索引范围内，指定数量的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成一定范围内指定数目的无重复数（mi：下限；ma：上限；num：数目）\n",
    "def random_without_same(mi, ma, num):\n",
    "    temp = range(mi, ma)  # 生成[mi,ma]之间的所有数\n",
    "    # random.shuffle(temp)  # 对数据乱序排列\n",
    "    temp = myShuffle(temp) \n",
    "    return temp[0:num]  # 返回数组前num个数字\n",
    "\n",
    "def randomCropSample(mi, ma, num,X_train,y_train):\n",
    "    # 生成“用于测试模型的训练集”，原始训练集太大，随机选取属于原始训练集一定数量（和验证集等大）的样本\n",
    "    randIndex_inTrain = random_without_same(mi,ma,num)\n",
    "\n",
    "    # 获取用于评估模型的训练集样本\n",
    "    X_train_evalua = []\n",
    "    y_train_evalua = []\n",
    "    for i in range(len(randIndex_inTrain)):\n",
    "        index = randIndex_inTrain[i]\n",
    "        X_train_evalua.append(X_train[index])\n",
    "        y_train_evalua.append(y_train[index])\n",
    "    return X_train_evalua,y_train_evalua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "垃圾样本：40951  阴性样本：41088  阳性样本：40911\n"
     ]
    }
   ],
   "source": [
    "nums = countSample(X_train)\n",
    "print \"垃圾样本：\"+str(nums[0])+\"  阴性样本：\"+str(nums[1])+\"  阳性样本：\"+str(nums[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型评价函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 调用接口计算“混淆矩阵”\n",
    "def analyResult(y_test,predicts):\n",
    "    \n",
    "    resultsAnaly = confusion_matrix(y_test,predicts)\n",
    "    print(\"混淆矩阵：\")\n",
    "    print(resultsAnaly) \n",
    "\n",
    "    accuracy = accuracy_score(y_test, predicts, normalize=True)\n",
    "    precision_tmp = precision_score(y_test, predicts,average=\"macro\")\n",
    "    f1_score_tmp = f1_score(y_test, predicts,average=\"macro\")\n",
    "    recall_score_tmp = recall_score(y_test,predicts,average='macro')\n",
    "\n",
    "    print(\"准确率：\"+str(accuracy))\n",
    "    print(\"精确率：\"+str(precision_tmp))\n",
    "    print(\"召回率：\"+str(recall_score_tmp))\n",
    "    print(\"f1值： \"+str(f1_score_tmp)) \n",
    "    return accuracy,precision_tmp,recall_score_tmp,f1_score_tmp\n",
    "\n",
    "# 分析混淆矩阵\n",
    "def analysisConfusionMatrix(ConfusionMatrix):\n",
    "    result = ConfusionMatrix\n",
    "    \n",
    "    # 垃圾污染阳性\n",
    "    two = float(sum(result[0][2:3]))/float(sum(result[0][:3])+1e-8)\n",
    "    \n",
    "    # 阴性污染阳性\n",
    "    three = float(sum(result[1][2:3]))/float(sum(result[1][:3])+1e-8)\n",
    "    \n",
    "    # 阳性丢失\n",
    "    one = float(sum(result[2][:2]))/float(sum(result[2][:3])+1e-8)\n",
    "    \n",
    "    print \"阳性丢失：\"+str(round(one,5))+\"  垃圾污染阳性：\"+str(round(two,5))+\"  阴性污染阳性：\"+str(round(three,5))\n",
    "    return one,two,three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义F_beta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析：对于“异常类别”，和查准率相比较，查全率更加重要（查全率高才能降低假阴性率），所以beta应该大于一。 \n",
    "#       但是也不能一味地追求查全率，查准率也有一定的重要性，所以使用使用“异常类别”的f_beta_score系数作为模型评价指标。\n",
    "\n",
    "def F_beta_score(confusionMatrix,beta):  # confusionMatrix二分类任务混淆矩阵,beta为异常类别f1score系数。\n",
    "    tp = confusionMatrix[0][0]\n",
    "    fn = confusionMatrix[0][1]\n",
    "    fp = confusionMatrix[1][0]\n",
    "    tn = confusionMatrix[1][1]\n",
    "    \n",
    "    # 计算“正常类别”的F_beta值\n",
    "    # p_normal = tp/(tp+fp)\n",
    "    # r_normal = tp/(tp+fn)\n",
    "    # f_beta1 = (1+beta1*beta1)*p_normal*r_normal/((beta1*beta1*p_normal)+r_normal)\n",
    "    \n",
    "    # 计算“异常类别”的F_beta值\n",
    "    p_ab = tn/(tn+fn+0.00000001)\n",
    "    r_ab = tn/(tn+fp+0.00000001)\n",
    "    f_beta2 = (1+beta*beta)*p_ab*r_ab/((beta*beta*p_ab)+r_ab+0.00000001)\n",
    "    \n",
    "    return f_beta2\n",
    "\n",
    "# 合并三分类混淆矩阵中“正常类别”和“垃圾类别”，然后调用上述F_beta_score\n",
    "def F_beta_score_MergeNormalAndJunk(confusionMatrix,beta):\n",
    "    newConfusionMatrix = []\n",
    "    tp = confusionMatrix[0][0]+confusionMatrix[0][1]+confusionMatrix[1][0]+confusionMatrix[1][1]\n",
    "    fn = confusionMatrix[0][2]+confusionMatrix[1][2]\n",
    "    fp = confusionMatrix[2][0]+confusionMatrix[2][1]\n",
    "    tn = confusionMatrix[2][2]\n",
    "    \n",
    "    newConfusionMatrix_one = []\n",
    "    newConfusionMatrix_one.append(tp)\n",
    "    newConfusionMatrix_one.append(fn)\n",
    "    \n",
    "    newConfusionMatrix_two = []\n",
    "    newConfusionMatrix_two.append(fp)\n",
    "    newConfusionMatrix_two.append(tn)\n",
    "    \n",
    "    newConfusionMatrix.append(newConfusionMatrix_one)\n",
    "    newConfusionMatrix.append(newConfusionMatrix_two)\n",
    "    \n",
    "    return F_beta_score(newConfusionMatrix,beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取ASCUS类别样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASCUS类别样本数据量为：\n",
      "35476\n",
      "35476\n"
     ]
    }
   ],
   "source": [
    "# 获取ASCUS类别样本\n",
    "def getSamplesDir_Ascus():   \n",
    "    imageAscus_dir = pwd+\"TBS3/ASCUS/Positive\"\n",
    "\n",
    "    # 获取所有样本名称 \n",
    "    listAscus = os.listdir(imageAscus_dir)\n",
    "    \n",
    "    # 路径+名称 \n",
    "    listAscus = [os.path.join(imageAscus_dir,listAscus[i]) for i in range(len(listAscus))]  \n",
    "    return listAscus\n",
    "\n",
    "\n",
    "X_ascus = getSamplesDir_Ascus()\n",
    "y_ascus = [[0,1,1]]*len(X_ascus)\n",
    "\n",
    "print \"ASCUS类别样本数据量为：\"\n",
    "print len(X_ascus)\n",
    "print len(y_ascus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型训练函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成训练集子集，用于分析模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "垃圾样本：6546  阴性样本：6621  阳性样本：6570\n"
     ]
    }
   ],
   "source": [
    "X_train_evalua,y_train_evalua=randomCropSample(0,len(X_train),len(X_val),X_train,y_train)\n",
    "\n",
    "nums = countSample(X_train_evalua)\n",
    "print \"垃圾样本：\"+str(nums[0])+\"  阴性样本：\"+str(nums[1])+\"  阳性样本：\"+str(nums[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "垃圾样本：9805  阴性样本：5802  阳性样本：4130\n",
      "垃圾样本：4987  阴性样本：5461  阳性样本：3260\n"
     ]
    }
   ],
   "source": [
    "nums = countSample(X_val)\n",
    "print \"垃圾样本：\"+str(nums[0])+\"  阴性样本：\"+str(nums[1])+\"  阳性样本：\"+str(nums[2])\n",
    "\n",
    "nums = countSample(X_test)\n",
    "print \"垃圾样本：\"+str(nums[0])+\"  阴性样本：\"+str(nums[1])+\"  阳性样本：\"+str(nums[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Build\n",
    "import tensorflow as tf  \n",
    "\n",
    "def trainFun(start,batch_size=64,weight_decay=1e-8,lr=1e-4,epochs=80,firstPred=True,modelPath=pwd+'jsL/keras_model/',loadmodel=None,modelname=None,reDir='info'):\n",
    "    # 首先情况重定向文件\n",
    "    f=open(reDir+'/info.txt','w+')\n",
    "    print >> f,\"\" \n",
    "    f.close() \n",
    "    #  每次运行都将图重新清空\n",
    "    tf.reset_default_graph()  \n",
    "    batch_size = batch_size   \n",
    "     # 实例化模型\n",
    "    model = Build.MobileNetV1(class_num=3)   \n",
    "    # 构造正则化损失函数\n",
    "    cost = model.cost  # 基本的交叉熵损失函数\n",
    "    l2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])   # L2正则化 \n",
    "    weight_decay = weight_decay  # 权重衰减系数\n",
    "    if weight_decay!=None:\n",
    "        cost = cost + l2_loss*weight_decay \n",
    "\n",
    "    # 表示训练多少个batch对应一epoch \n",
    "    epoch = int(len(X_train)/batch_size+1e-9)  \n",
    "    \n",
    "    # 配置tensorflow实用显存的方式\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)  # 自动选择可以运行的设备 \n",
    "    config.gpu_options.allow_growth = True  # Gpu内存按需增长 \n",
    "    # config.gpu_options.per_process_gpu_memory_fraction = 0.3 \n",
    "    saver = tf.train.Saver(max_to_keep=1) # 模型保存器 \n",
    "    \n",
    "    # 使用SGD优化器，并且使用权重衰减\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(lr,global_step,10000,0.92,staircase=True)\n",
    "    # train = tf.train.GradientDescentOptimizer(learning_rate).minimize(model.cost,global_step=global_step)\n",
    "    train = tf.train.AdamOptimizer(learning_rate).minimize(model.cost,global_step=global_step) \n",
    "    # train = tf.train.MomentumOptimizer(learning_rate,momentum=0.9,use_nesterov=True).minimize(model.cost,global_step=global_step) \n",
    "\n",
    "    with tf.Session(config = config) as sess:    \n",
    "        sess.run(tf.global_variables_initializer())  # 初始化变量  \n",
    "        \n",
    "        # 判断是否加载与训练模型\n",
    "        if loadmodel!=None:\n",
    "            saver = tf.train.import_meta_graph(loadmodel+modelname)\n",
    "            saver.restore(sess,tf.train.latest_checkpoint(loadmodel))\n",
    "\n",
    "        val_accuracy_list = []  # 绘制验证集准确率变化曲线\n",
    "        val_f1_list = []  # 绘制验证集f1_score变化曲线\n",
    "        train_accuracy_list = []  # 绘制训练集准确率变化曲线\n",
    "        cost_list_train = []\n",
    "        cost_list_val = []\n",
    "\n",
    "        val_f_beta_score_max = -1 # 记录val_f1的最大值\n",
    "        val_f1_max = -1 # 记录val_f1的最大值\n",
    "        # 设置训练轮数(设置迭代400epoch)\n",
    "        STEPS = int(epoch*epochs)  \n",
    "        print \"STEPS:\"+str(STEPS)\n",
    "        for i in range(start,STEPS+start):   # 表示从索引为start开始训练样本（主要是为了均衡二次训练时样本被训练的机会）\n",
    "            \n",
    "            utils.printRd(\"............................第\"+str(i)+\"个batch............................\",reDir)  \n",
    "            if i%(int(0.2*epoch))==0 and (i!=0 or firstPred==True):  # 判断在进行训练之前是否进行预测。 \n",
    "                \n",
    "                # 处理ASCUS类别数据\n",
    "                ascus_predicts,time_ascus = utils.myPredicts(sess,model.prob,model.inputs,model.is_training,X_ascus,batch_size,reDir)\n",
    "                ascus_evalua_realLabel = utils.getRealLabel(y_ascus)\n",
    "                ascus_predicts_realLabel = utils.getRealLabel2(ascus_predicts,countSample(X_train)) \n",
    "                ascus_confusion_matrix = confusion_matrix(ascus_evalua_realLabel,ascus_predicts_realLabel) \n",
    "                print \"ASCUS类别混淆矩阵为：\"\n",
    "                print np.array(ascus_confusion_matrix)\n",
    "                analyResult(ascus_evalua_realLabel,ascus_predicts_realLabel)\n",
    "                print \"阳性类别F1分数：\"+str(round(F_beta_score_MergeNormalAndJunk(ascus_confusion_matrix,1),5))\n",
    "                analysisConfusionMatrix(ascus_confusion_matrix)\n",
    "                \n",
    "                \n",
    "                # 处理测试集\n",
    "                test_predicts,time_test = utils.myPredicts(sess,model.prob,model.inputs,model.is_training,X_test,batch_size,reDir)\n",
    "                test_realLabel = utils.getRealLabel(y_test)\n",
    "                test_predicts_realLabel = utils.getRealLabel2(test_predicts,countSample(X_train)) \n",
    "                test_confusion_matrix = confusion_matrix(test_realLabel,test_predicts_realLabel) \n",
    "                print \"测试集混淆矩阵为：\"\n",
    "                print np.array(test_confusion_matrix)\n",
    "                analyResult(test_realLabel,test_predicts_realLabel)\n",
    "                print \"阳性类别F1分数：\"+str(round(F_beta_score_MergeNormalAndJunk(test_confusion_matrix,1),5))\n",
    "                analysisConfusionMatrix(test_confusion_matrix)\n",
    "                #                 # 将测试集中预测错误的样本写入到文件中\n",
    "                #                 for i in range(len(test_realLabel)):\n",
    "                #                     # if test_realLabel[i]!=test_predicts_realLabel[i]:\n",
    "                #                     if test_realLabel[i]!=test_predicts_realLabel[i] and (test_realLabel[i]==2 or test_predicts_realLabel[i]==2):  \n",
    "\n",
    "                #                         value = X_test[i]\n",
    "                #                         if value.find('/notebooks/17_LJS/TbsData/TBS3/Test/Negative/')>=0:\n",
    "                #                             value = value.replace('/notebooks/17_LJS/TbsData/TBS3/Test/Negative/','')\n",
    "                #                         elif value.find('/notebooks/17_LJS/TbsData/TBS3/Test/Positive/')>=0:\n",
    "                #                             value = value.replace('/notebooks/17_LJS/TbsData/TBS3/Test/Positive/','')\n",
    "                #                         elif value.find('/notebooks/17_LJS/TbsData/TBS3/Test/Junk/')>=0:\n",
    "                #                             value = value.replace('/notebooks/17_LJS/TbsData/TBS3/Test/Junk/','')\n",
    "                #                         else:\n",
    "                #                             print \"发生了错误！\"\n",
    "\n",
    "                #                         utils.printRd(value,'info_test')\n",
    "                #                 break\n",
    "                \n",
    "                \n",
    "                # 处理验证集\n",
    "                val_predicts,time_val = utils.myPredicts(sess,model.prob,model.inputs,model.is_training,X_val,batch_size,reDir)\n",
    "                val_realLabel = utils.getRealLabel(y_val)\n",
    "                val_predicts_realLabel = utils.getRealLabel2(val_predicts,countSample(X_train)) \n",
    "                val_confusion_matrix = confusion_matrix(val_realLabel,val_predicts_realLabel) \n",
    "                print \"验证集混淆矩阵为：\"\n",
    "                print np.array(val_confusion_matrix)\n",
    "                analyResult(val_realLabel,val_predicts_realLabel)\n",
    "                print \"阳性类别F1分数：\"+str(round(F_beta_score_MergeNormalAndJunk(val_confusion_matrix,1),5))\n",
    "                analysisConfusionMatrix(val_confusion_matrix)\n",
    "                \n",
    "                \n",
    "                # 处理训练集\n",
    "                train_predicts,time_train = utils.myPredicts(sess,model.prob,model.inputs,model.is_training,X_train,batch_size,reDir)\n",
    "                train_evalua_realLabel = utils.getRealLabel(y_train)\n",
    "                train_predicts_realLabel = utils.getRealLabel2(train_predicts,countSample(X_train)) \n",
    "                train_confusion_matrix = confusion_matrix(train_evalua_realLabel,train_predicts_realLabel) \n",
    "                print \"训练集混淆矩阵为：\"\n",
    "                print np.array(train_confusion_matrix)\n",
    "                analyResult(train_evalua_realLabel,train_predicts_realLabel)\n",
    "                print \"阳性类别F1分数：\"+str(round(F_beta_score_MergeNormalAndJunk(train_confusion_matrix,1),5))\n",
    "                analysisConfusionMatrix(train_confusion_matrix)\n",
    "                \n",
    "                time_total = time_ascus+time_train+time_test+time_val  # 预测总时长\n",
    "                num_total = len(X_train)+len(X_val)+len(X_test)+len(X_ascus)  # 所预测的样本总量\n",
    "                predict_efficiency = num_total/time_total  # 求单位时间内模型预测细胞的数量\n",
    "                print \"预测总时长为：\"+str(time_total)\n",
    "                print \"预测细胞总数量为：\"+str(num_total)\n",
    "                print \"模型单位时间内预测细胞数量为：\"+str(round(predict_efficiency,0))\n",
    "                break;\n",
    "                \n",
    "                \n",
    "                # 记录“训练集准确率”、“验证集准确率”、“验证集F1”  \n",
    "                train_predicts,_ = utils.myPredicts(sess,model.prob,model.inputs,model.is_training,X_train_evalua,batch_size,reDir)  \n",
    "                val_predicts,_ = utils.myPredicts(sess,model.prob,model.inputs,model.is_training,X_val,batch_size,reDir)\n",
    "                \n",
    "                # 求真实标签\n",
    "                train_evalua_realLabel = utils.getRealLabel(y_train_evalua)\n",
    "                train_predicts_realLabel = utils.getRealLabel2(train_predicts,countSample(X_train))\n",
    "                # print \"train_predicts_realLabel:\"\n",
    "                # print train_predicts_realLabel\n",
    "                \n",
    "                val_realLabel = utils.getRealLabel(y_val)\n",
    "                val_predicts_realLabel = utils.getRealLabel2(val_predicts,countSample(X_train))\n",
    "\n",
    "                # 计算“准确率”等 \n",
    "                train_accuracy = accuracy_score(train_evalua_realLabel,train_predicts_realLabel, normalize=True)\n",
    "                val_accuracy = accuracy_score(val_realLabel,val_predicts_realLabel, normalize=True)\n",
    "                val_f1 = f1_score(val_realLabel,val_predicts_realLabel,average=\"macro\") \n",
    "                train_confusion_matrix = confusion_matrix(train_evalua_realLabel,train_predicts_realLabel)\n",
    "                val_confusion_matrix = confusion_matrix(val_realLabel,val_predicts_realLabel)\n",
    "                \n",
    "                # 修改模型评价指标为“异常类别”的f_beta_score\n",
    "                val_f_beta_score = F_beta_score_MergeNormalAndJunk(val_confusion_matrix,beta=1)  # beta=2，表示“异常类别”查全率的重要性是查准率的4倍\n",
    "                \n",
    "                # 训练集交叉熵\n",
    "                cross_entropy_train = sess.run(-tf.reduce_mean(y_train_evalua*tf.log(tf.clip_by_value(train_predicts,1e-10,1.0))))\n",
    "                cross_entropy_val = sess.run(-tf.reduce_mean(y_val*tf.log(tf.clip_by_value(val_predicts,1e-10,1.0))))\n",
    "                \n",
    "                # 对各个参数只保留五位小数（后面的位四舍五入处理）\n",
    "                train_accuracy = round(train_accuracy,5)\n",
    "                val_accuracy = round(val_accuracy,5)\n",
    "                cross_entropy_train = round(cross_entropy_train,5)\n",
    "                cross_entropy_val = round(cross_entropy_val,5)\n",
    "                val_f1 = round(val_f1,5) \n",
    "                val_f_beta_score = round(val_f_beta_score,5)\n",
    "\n",
    "                utils.printRd(\"训练集混淆矩阵为：\",reDir)\n",
    "                utils.printRd(np.array(train_confusion_matrix),reDir)\n",
    "                utils.printRd(analysisConfusionMatrix(train_confusion_matrix),reDir)\n",
    "                utils.printRd(\"验证集混淆矩阵为：\",reDir)\n",
    "                utils.printRd(np.array(val_confusion_matrix),reDir)\n",
    "                utils.printRd(analysisConfusionMatrix(val_confusion_matrix),reDir)\n",
    "\n",
    "                utils.printRd(\"第\"+str(i)+\"次迭代：\"+\"train_accuracy=\"+str(train_accuracy)+\" val_accuracy=\"+str(val_accuracy)+\" cost=\"+str(cross_entropy_train)+\" val_f1:\"+str(val_f1)+ \" val_f_beta_score:\"+str(val_f_beta_score),reDir) \n",
    " \n",
    "                train_accuracy_list.append(train_accuracy)\n",
    "                val_accuracy_list.append(val_accuracy)\n",
    "                val_f1_list.append(val_f1) \n",
    "                cost_list_train.append(cross_entropy_train)\n",
    "                cost_list_val.append(cross_entropy_val)\n",
    "                utils.showPerformCurve(val_accuracy_list,val_f1_list,train_accuracy_list,reDir+'/perform.jpg') # 显示曲线 \n",
    "                utils.showPerformCurve_cost(cost_list_train,cost_list_val,reDir+'/cost.jpg') # 显示曲线 \n",
    "                \n",
    "                if val_f1>val_f1_max:\n",
    "                    val_f1_max = val_f1\n",
    "                    utils.printRd(\"val_f1最大值更新：\"+str(val_f1_max),reDir)\n",
    "\n",
    "                # 修改模型保存条件为：每次epoch判断一次val_f1是否大于val_f1_max,假如满足条件就保存一次模型\n",
    "                if val_f_beta_score>val_f_beta_score_max:  # 这样做也就相当于earlystoping。\n",
    "                    # vgg.save_npy(sess,pwd+'jsL/keras_model/vggPara-save_train_l2正则化解决过拟合.npy')     \n",
    "                    # 因为模型中使用了BN层，参数无法提取，所以替换使用Saver.save函数保存模型\n",
    "                    saver.save(sess, modelPath)  # 保存模型  \n",
    "                    val_f_beta_score_max = val_f_beta_score\n",
    "                    utils.printRd(\"model saved!\",reDir)\n",
    "                else:\n",
    "                    utils.printRd(\"model not saved!\",reDir)  \n",
    "\n",
    "            # 每次选取batch_size个样本进行训练\n",
    "            # 梯度下降训练模型参数\n",
    "            start = (i*batch_size)%len(X_train)\n",
    "            end = min(start+batch_size,len(X_train)) \n",
    "            xxx = utils.getMiniBatch4Train(X_train[start:end])\n",
    "            sess.run(train, feed_dict={model.inputs: xxx, model.labels: y_train[start:end], model.is_training: True})  \n",
    "        sess.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "initial_depthwise = tf.truncated_normal([4, 4, 4, 4], 0.0, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标记：\n",
      "Tensor(\"Placeholder:0\", shape=(?, 128, 128, 3), dtype=float32)\n",
      "Tensor(\"Placeholder_1:0\", shape=(?, 3), dtype=float32)\n",
      "Tensor(\"Placeholder_2:0\", dtype=bool)\n",
      "start-shape:(?, 128, 128, 3)\n",
      "the 1th stage-shape：(?, 64, 64, 32)\n",
      "the 2th stage-shape：(?, 32, 32, 32)\n",
      "the 3th stage-shape：(?, 16, 16, 64)\n",
      "the 4th stage-shape：(?, 8, 8, 64)\n",
      "Tensor(\"prob:0\", shape=(?, 3), dtype=float32)\n",
      "the 5th stage-shape：(?, 3)\n",
      "INFO:tensorflow:Restoring parameters from /notebooks/17_LJS/model2019/Pangbo/model_new\n",
      "STEPS:7200\n",
      "ASCUS类别混淆矩阵为：\n",
      "[[    0     0     0]\n",
      " [    0     0     0]\n",
      " [  232 10704 24540]]\n",
      "混淆矩阵：\n",
      "[[    0     0     0]\n",
      " [    0     0     0]\n",
      " [  232 10704 24540]]\n",
      "准确率：0.6917352576389671\n",
      "精确率：0.3333333333333333\n",
      "召回率：0.23057841921298905\n",
      "f1值： 0.27259397494001597\n",
      "阳性类别F1分数：0.81778\n",
      "阳性丢失：0.30826  垃圾污染阳性：0.0  阴性污染阳性：0.0\n",
      "测试集混淆矩阵为：\n",
      "[[4135  810   42]\n",
      " [  55 5160  246]\n",
      " [  10  154 3096]]\n",
      "混淆矩阵：\n",
      "[[4135  810   42]\n",
      " [  55 5160  246]\n",
      " [  10  154 3096]]\n",
      "准确率：0.9039247154946017\n",
      "精确率：0.9140013237623617\n",
      "召回率：0.907910315463588\n",
      "f1值： 0.907653605253462\n",
      "阳性类别F1分数：0.93197\n",
      "阳性丢失：0.05031  垃圾污染阳性：0.00842  阴性污染阳性：0.04505\n",
      "验证集混淆矩阵为：\n",
      "[[9447  328   30]\n",
      " [ 108 5373  321]\n",
      " [   9  144 3977]]\n",
      "混淆矩阵：\n",
      "[[9447  328   30]\n",
      " [ 108 5373  321]\n",
      " [   9  144 3977]]\n",
      "准确率：0.9523737143436186\n",
      "精确率：0.9419713431773559\n",
      "召回率：0.9508339969310223\n",
      "f1值： 0.9461762482499522\n",
      "阳性类别F1分数：0.94041\n",
      "阳性丢失：0.03705  垃圾污染阳性：0.00306  阴性污染阳性：0.05533\n",
      "训练集混淆矩阵为：\n",
      "[[39579  1259   113]\n",
      " [  332 39867   889]\n",
      " [   16  1457 39438]]\n",
      "混淆矩阵：\n",
      "[[39579  1259   113]\n",
      " [  332 39867   889]\n",
      " [   16  1457 39438]]\n",
      "准确率：0.9669296461976413\n",
      "精确率：0.967575108071619\n",
      "召回率：0.966924950869827\n",
      "f1值： 0.9670854472066664\n",
      "阳性类别F1分数：0.96958\n",
      "阳性丢失：0.036  垃圾污染阳性：0.00276  阴性污染阳性：0.02164\n",
      "预测总时长为：92.1592943668\n",
      "预测细胞总数量为：191871\n",
      "模型单位时间内预测细胞数量为：2082.0\n"
     ]
    }
   ],
   "source": [
    "loadmodel = '/notebooks/17_LJS/model2019/Pangbo/'\n",
    "modelname = 'model_new.meta'\n",
    "reDir = 'info_test' \n",
    "\n",
    "modelPath='/notebooks/17_LJS/model2019/Pangbo/model_new'\n",
    "trainFun(0,batch_size=256,weight_decay=1e-6,lr=1e-3,epochs=15,firstPred=True,modelPath=modelPath,loadmodel=loadmodel,modelname=modelname,reDir=reDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript \n",
    "Jupyter.notebook.session.restart() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x  = 0.01\n",
    "# for i in range(15):\n",
    "#     print \"x:\"+str(x)\n",
    "#     x = x*0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
